import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
%matplotlib inline
from sklearn import metrics

data=pd.read_csv("C:\\Users\\JAYALAKSHMI\\Downloads\\Iris.csv")
features = ['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']

# Create Features matrix
x=data.loc[:,features].values
y=data.Species
x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=0)
clf=DecisionTreeClassifier(max_depth=2,random_state=0)
clf.fit(x_train,y_train)
clf.predict(x_test[0:1])
score=clf.score(x_test,y_test)
print(score)
print(metrics.classification_report(y_test,clf.predict(x_test)))
max_depth_range=list(range(1,6))
accuracy=[]

for depth in max_depth_range:
    clf=DecisionTreeClassifier(max_depth=depth,random_state=0)
    clf.fit(x_train,y_train)
    score=clf.score(x_test,y_test)
    accuracy.append(score)

# Plotting accuracy score depth wise

fig,ax=plt.subplots(nrows=1,ncols=1,figsize=(10,17));
ax.plot(max_depth_range,accuracy,lw=2,color='k')

ax.set_xlim([1,5])
ax.set_ylim([.50,1.00])
ax.grid(True,axis='both',zorder=0,linestyle=':',color='k')

# Decision tree visualization

ax.tick_params(labelsize=18)
ax.set_xticks([1,2,3,4,5])
ax.set_xlabel('max_depth',fontsize=24)
ax.set_ylabel('Accuracy',fontsize=24)
fig.tight_layout()
fig,axes=plt.subplots(nrows=1,ncols=1,figsize=(7,4),dpi=150)
tree.plot_tree(clf);
fn=['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)']
cn=['setosa','versicolor','virginica']

# Making Decision tree more interpretable

fig,axes=plt.subplots(nrows=1,ncols=1,figsize=(7,4),dpi=300)
tree.plot_tree(clf,feature_names=fn,class_names=cn,filled=True);
